# CL-CLIP


<div align="center">
<h3>Don't Stop Learning: Towards Continual Learning for the CLIP Model</h3>

[Yuxuan Ding](https://yxding95.github.io/)<sup>1</sup>, [Lingqiao Liu](https://scholar.google.com.au/citations?user=Y2xu62UAAAAJ&hl=en&oi=ao)<sup>3</sup>, Chunna Tian<sup>1</sup>, [Jingyuan Yang](https://jingyuanyy.github.io/)<sup>3</sup>, Haoxuan Ding<sup>4</sup>

<sup>1</sup> Xidian University, <sup>3</sup> The University of Adelaide, <sup>3</sup> Shenzhen University, <sup>4</sup> Northwestern Polytechnical University

[![arXiv](https://img.shields.io/badge/arXiv-<2207.09248>-<COLOR>.svg)](https://arxiv.org/abs/2207.09248)

</div>

## Todo

- [ ] release the code for CL-CLIP

## Acknowledgments

Our work is standing on the shoulders of giants. We want to thank the following contributors that our code is based on:

- open-source library of transformer models:
  - (ðŸ¤— Transformers) https://github.com/huggingface/diffusers


## Citation

If this work is helpful for your research, please consider citing the following BibTeX entry.

```
@article{ding2022don,
  title={Don't stop learning: Towards continual learning for the clip model},
  author={Ding, Yuxuan and Liu, Lingqiao and Tian, Chunna and Yang, Jingyuan and Ding, Haoxuan},
  journal={arXiv preprint arXiv:2207.09248},
  year={2022}
}
```

